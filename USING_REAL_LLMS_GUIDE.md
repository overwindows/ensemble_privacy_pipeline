# Using Real LLMs Guide

**Complete guide to switching from Mock LLMs to Real LLM APIs**

---

## â“ Your Questions Answered

### Question 1: What should I do if I want to leverage real LLMs?

**Answer**: Follow the steps below to use real LLM APIs (GPT-4, Claude, Gemini, etc.)

### Question 2: Is the benchmark targeted for LLMs?

**Answer**: **YES** - The benchmarks evaluate your **LLM-based pipeline**:
- They test how well your pipeline (which uses LLMs internally) protects privacy
- They measure if LLMs leak PII when processing sensitive data
- They validate your ensemble-consensus mechanism with LLMs

**The benchmarks don't test the LLMs themselves**, they test **your privacy pipeline that uses LLMs**.

---

## ðŸš€ How to Use Real LLMs

### Step 1: Install Required Packages

```bash
# For OpenAI (GPT-4, GPT-4-turbo)
pip install openai

# For Anthropic (Claude)
pip install anthropic

# For Google (Gemini)
pip install google-generativeai

# Install all at once
pip install openai anthropic google-generativeai
```

---

### Step 2: Set Up API Keys

```bash
# Option A: Environment variables (recommended)
export OPENAI_API_KEY='sk-...'
export ANTHROPIC_API_KEY='sk-ant-...'
export GOOGLE_API_KEY='...'

# Option B: Create .env file
cat > .env << EOF
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
EOF
```

---

### Step 3: Run with Real LLMs

#### Option A: Use the Example Script (Easiest)

```bash
# Run the pre-built example
python examples/real_llm_example.py
```

**What it does**:
- Uses real LLM APIs (GPT-4, Claude, etc.)
- Runs the full 4-step pipeline
- Shows API costs
- Validates privacy protection

**Expected output**:
```
ðŸ”¬ Running Ensemble Pipeline with REAL LLMs
================================================

Step 1: Redaction & Masking
âœ“ Masked 3 queries
âœ“ Masked 11 article titles

Step 3: Ensemble Evaluation (Real LLMs)
  Calling GPT-4...                    âœ“ (0.8s, $0.012)
  Calling Claude-3.5-Sonnet...        âœ“ (1.2s, $0.008)
  Calling GPT-4-turbo...              âœ“ (0.6s, $0.006)

Step 4: Consensus Aggregation
âœ“ Aggregated 3 model outputs

Final Output:
[
  {
    "ItemId": "diabetes-management",
    "QualityScore": 0.84,
    "QualityReason": "VeryStrong:MSNClicks+BingSearch"
  }
]

ðŸ’° Total Cost: $0.026 for 1 user
```

---

#### Option B: Modify Your Own Code

Replace `MockLLMEvaluator` with `RealLLMEvaluator`:

```python
# OLD CODE (Mock LLMs)
from src.pipeline import MockLLMEvaluator

evaluators = [
    MockLLMEvaluator("GPT-4", bias=0.0),
    MockLLMEvaluator("Claude-3.5", bias=0.05),
    MockLLMEvaluator("Gemini-Pro", bias=-0.03)
]

# NEW CODE (Real LLMs)
from examples.real_llm_example import RealLLMEvaluator

evaluators = [
    RealLLMEvaluator("gpt-4", api_key=os.getenv('OPENAI_API_KEY')),
    RealLLMEvaluator("claude-3-5-sonnet-20241022", api_key=os.getenv('ANTHROPIC_API_KEY')),
    RealLLMEvaluator("gpt-4-turbo", api_key=os.getenv('OPENAI_API_KEY'))
]

# Rest of the pipeline stays the same
all_results = []
for evaluator in evaluators:
    results = evaluator.evaluate_interest(masked_data, candidate_topics)
    all_results.append(results)

consensus = aggregator.aggregate_median(all_results)
```

---

### Step 4: Run Benchmarks with Real LLMs

The benchmarks can use either Mock or Real LLMs:

```bash
# With Mock LLMs (default, free, fast)
python benchmarks/public_datasets.py --benchmark ai4privacy --num_samples 100

# With Real LLMs (add --use-real-llms flag)
# NOTE: This would require modifying the benchmark script to accept this flag
python benchmarks/public_datasets.py --benchmark ai4privacy --num_samples 100 --use-real-llms
```

**Currently**, benchmarks use **Mock LLMs by default** because:
- âœ… Free (no API costs)
- âœ… Fast (no network latency)
- âœ… Reproducible results
- âœ… Tests the **mechanism**, not specific LLM behavior

---

## ðŸŽ¯ Understanding What Benchmarks Test

### What Benchmarks Do

The benchmarks test **YOUR PIPELINE**, not the LLMs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          YOUR PRIVACY PIPELINE                   â”‚
â”‚                                                  â”‚
â”‚  Input: Sensitive Data                          â”‚
â”‚    â†“                                            â”‚
â”‚  Step 1: Redaction (your code)                  â”‚
â”‚    â†“                                            â”‚
â”‚  Step 3: Ensemble LLMs (Mock or Real)          â”‚  â† Benchmarks test this!
â”‚    â†“                                            â”‚
â”‚  Step 4: Consensus (your code)                  â”‚
â”‚    â†“                                            â”‚
â”‚  Output: Safe JSON                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Benchmark checks:
    âœ“ Does output leak PII?
    âœ“ Can attacker reconstruct input?
    âœ“ Is privacy preserved?
```

---

### What Benchmarks Measure

| Benchmark | Tests | Target |
|-----------|-------|--------|
| **ai4privacy/pii-masking-200k** | PII leakage in outputs | Your pipeline's output |
| **PII-Bench** | 55 PII categories protection | Your pipeline's output |
| **PrivacyXray** | Profile reconstruction attacks | Your pipeline's mechanism |
| **Canary Exposure** | Do secrets leak? | Your pipeline's aggregation |
| **Membership Inference** | Can attacker tell if data was used? | Your pipeline's privacy |
| **Attribute Inference** | Can attacker infer attributes? | Your pipeline's suppression |

**They DON'T test**:
- âŒ LLM quality/accuracy (that's utility testing)
- âŒ Which LLM is best
- âŒ LLM-specific vulnerabilities

**They DO test**:
- âœ… Your redaction mechanism
- âœ… Your consensus aggregation
- âœ… Your privacy boundary
- âœ… Whether your approach prevents leakage

---

## ðŸ’° Cost Comparison

### Mock LLMs (Current Default)

```
Cost: $0.00
Time: ~2 seconds for full pipeline
Samples: Process 1000+ samples easily
```

**Use for**:
- âœ… Development and testing
- âœ… Benchmarking (mechanism validation)
- âœ… Demonstrations
- âœ… CI/CD pipelines

---

### Real LLMs

```
Cost per user (5-model ensemble):
  - GPT-4: $0.03/1K tokens Ã— 3 calls = $0.01
  - Claude-3.5: $0.003/1K tokens Ã— 3 calls = $0.001
  - GPT-4-turbo: $0.01/1K tokens Ã— 3 calls = $0.003
  - Gemini-Pro: $0.0005/1K tokens Ã— 3 calls = $0.0002
  - Llama (self-hosted): Free

Total: ~$0.015 - $0.05 per user

Time: ~2-5 seconds per user (with parallel calls)
Samples: Process 100-1000 samples ($1.50 - $50)
```

**Use for**:
- âœ… Production deployment
- âœ… Real-world validation
- âœ… Performance testing
- âœ… Client demonstrations

---

## ðŸ”§ Recommended Setup

### For Development (Mock LLMs)

```python
# src/pipeline.py (already configured)
evaluators = [
    MockLLMEvaluator("GPT-4", bias=0.0),
    MockLLMEvaluator("Claude-3.5", bias=0.05),
    MockLLMEvaluator("Gemini-Pro", bias=-0.03),
    MockLLMEvaluator("Llama-3", bias=0.02),
    MockLLMEvaluator("Mistral-Large", bias=-0.01)
]
```

**Run**:
```bash
python src/pipeline.py  # Free, fast
```

---

### For Production (Real LLMs)

```python
# examples/real_llm_example.py (already configured)
from examples.real_llm_example import RealLLMEvaluator

evaluators = [
    RealLLMEvaluator("gpt-4", api_key=os.getenv('OPENAI_API_KEY')),
    RealLLMEvaluator("claude-3-5-sonnet-20241022", api_key=os.getenv('ANTHROPIC_API_KEY')),
    RealLLMEvaluator("gpt-4-turbo", api_key=os.getenv('OPENAI_API_KEY'))
]
```

**Run**:
```bash
export OPENAI_API_KEY='sk-...'
export ANTHROPIC_API_KEY='sk-ant-...'
python examples/real_llm_example.py  # Costs ~$0.03/user
```

---

### For Benchmarks (Mock LLMs Recommended)

```bash
# Test privacy mechanism with mock LLMs (free, fast)
python benchmarks/public_datasets.py --benchmark ai4privacy --num_samples 1000

# Why mock is sufficient for benchmarks:
# âœ“ Tests YOUR redaction mechanism
# âœ“ Tests YOUR consensus aggregation
# âœ“ Tests privacy boundary enforcement
# âœ“ Mock LLMs follow the same logic as real LLMs
```

---

## ðŸ“Š Benchmark Target Clarification

### Q: "Is the benchmark targeted for LLMs?"

**A: YES, but it tests YOUR PIPELINE that USES LLMs, not the LLMs themselves.**

Think of it this way:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  What Benchmarks Test:                 â”‚
â”‚                                        â”‚
â”‚  YOUR APPROACH:                        â”‚
â”‚  â”œâ”€ Redaction mechanism                â”‚  â† Your contribution
â”‚  â”œâ”€ Ensemble strategy                  â”‚  â† Your contribution
â”‚  â”œâ”€ Consensus voting                   â”‚  â† Your contribution
â”‚  â””â”€ Privacy boundary enforcement       â”‚  â† Your contribution
â”‚                                        â”‚
â”‚  USING:                                â”‚
â”‚  â””â”€ LLMs (Mock or Real)                â”‚  â† Tool (interchangeable)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Analogy**:
- Benchmark tests if your **car** is safe (your approach)
- Engine (LLM) can be gas or electric (Mock or Real)
- Safety features (redaction, consensus) are what matter
- Engine type doesn't change safety test results

---

## ðŸŽ¯ When to Use Mock vs Real LLMs

| Scenario | Use Mock LLMs | Use Real LLMs |
|----------|---------------|---------------|
| **Development** | âœ… Yes | âŒ Too slow/expensive |
| **Testing** | âœ… Yes | âŒ Unnecessary |
| **Benchmarking** | âœ… Yes | âš ï¸ Optional (expensive) |
| **Privacy Validation** | âœ… Yes | âš ï¸ Same results, higher cost |
| **Utility Validation** | âŒ Need real for accuracy | âœ… Yes |
| **Production** | âŒ Too simplistic | âœ… Yes |
| **Client Demo** | âš ï¸ OK for quick demo | âœ… Better for proof |
| **Research Paper** | âœ… Yes (mechanism focus) | âš ï¸ Optional (small sample) |

---

## ðŸš€ Quick Start Commands

### Demo (Mock LLMs)

```bash
# See the approach in action (free, 2 seconds)
python src/pipeline.py
```

---

### Demo (Real LLMs)

```bash
# See with actual APIs (costs ~$0.03, 5 seconds)
export OPENAI_API_KEY='sk-...'
python examples/real_llm_example.py
```

---

### Benchmark (Mock LLMs - Recommended)

```bash
# Validate privacy mechanism (free, ~1 minute for 1000 samples)
python benchmarks/public_datasets.py --benchmark ai4privacy --num_samples 1000
```

---

### Benchmark (Real LLMs - Expensive)

```bash
# Validate with real APIs (costs ~$1.50-$50 for 100-1000 samples)
# NOTE: Current benchmarks use Mock by default
# You'd need to modify benchmark code to use Real LLMs
python benchmarks/public_datasets.py --benchmark ai4privacy --num_samples 100
```

---

## ðŸ“ Summary

### To Use Real LLMs:

1. **Install packages**: `pip install openai anthropic google-generativeai`
2. **Set API keys**: `export OPENAI_API_KEY='sk-...'`
3. **Run example**: `python examples/real_llm_example.py`
4. **Or modify your code**: Replace `MockLLMEvaluator` with `RealLLMEvaluator`

### About Benchmarks:

- âœ… **YES**, benchmarks test your **LLM-based pipeline**
- âœ… They test your **privacy mechanism** (redaction, ensemble, consensus)
- âœ… They measure if **LLMs in your pipeline** leak PII
- âœ… Mock LLMs are **sufficient** for benchmarking (tests mechanism, not LLM quality)
- âš ï¸ Real LLMs are **optional** for benchmarks (same results, higher cost)
- âœ… Real LLMs are **required** for production and utility validation

### Key Insight:

**Your contribution is the privacy mechanism (redaction + ensemble + consensus), not the LLMs themselves.**

The benchmarks validate that **YOUR MECHANISM** works, regardless of whether you use Mock or Real LLMs.

---

**Ready to use real LLMs?** Start with:
```bash
export OPENAI_API_KEY='sk-...'
python examples/real_llm_example.py
```

**Date**: 2025-01-14
